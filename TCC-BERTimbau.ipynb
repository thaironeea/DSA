{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be3043b-8f3e-4997-819c-219e09848266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis do DataFrame \"ws_agencia_sebrae_noticias.xlsx\" com o modelo BERTimabu (BASE)\n",
    "\n",
    "# Importar as bibliotecas necessárias\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import warnings\n",
    "\n",
    "# Caminho do arquivo no computador\n",
    "caminho_arquivo = r'C:\\Users\\Thairone Almeida\\Google Drive\\Thairone\\Hashtag Treinamentos\\Python Impressionador\\Script\\MBA_USP_ESALQ\\Dados até 30 06 2024\\ws_agencia_sebrae_noticias.xlsx'\n",
    "\n",
    "# Caminhos dos modelos BERTimbau\n",
    "caminho_bertimbau_base = r'C:\\Users\\Thairone Almeida\\Google Drive\\Thairone\\Hashtag Treinamentos\\Python Impressionador\\Script\\MBA_USP_ESALQ\\BERTimbau_base_GoEmotions_portuguese'\n",
    "caminho_bertimbau_large = r'C:\\Users\\Thairone Almeida\\Google Drive\\Thairone\\Hashtag Treinamentos\\Python Impressionador\\Script\\MBA_USP_ESALQ\\BERTimbau_large_GoEmotions_portuguese'\n",
    "\n",
    "# Carregar os dados do arquivo Excel\n",
    "df = pd.read_excel(caminho_arquivo)\n",
    "\n",
    "# Mapeamento de BERTimbau Base\n",
    "mapa_sentimentos_base = {\n",
    "    0: \"admiração\", 1: \"diversão\", 2: \"raiva\", 3: \"aborrecimento\", 4: \"aprovação\", 5: \"zelo\", 6: \"confusão\", 7: \"curiosidade\", 8: \"desejo\", 9: \"decepção\", 10: \"desaprovação\", 11: \"nojo\", 12: \"constrangimento\", 13: \"entusiasmo\", 14: \"medo\", 15: \"gratidão\", 16: \"luto\", 17: \"alegria\", 18: \"amor\", 19: \"nervosismo\", 20: \"otimismo\", 21: \"orgulho\", 22: \"percepção\", 23: \"alívio\", 24: \"remorso\", 25: \"tristeza\", 26: \"surpresa\", 27: \"neutro\"\n",
    "}\n",
    "\n",
    "# Mapeamento de BERTimbau Large\n",
    "mapa_sentimentos_large = {\n",
    "    0: \"admiração\", 1: \"diversão\", 2: \"raiva\", 3: \"aborrecimento\", 4: \"aprovação\", 5: \"zelo\", 6: \"confusão\", 7: \"curiosidade\", 8: \"desejo\", 9: \"decepção\", 10: \"desaprovação\", 11: \"nojo\", 12: \"constrangimento\", 13: \"entusiasmo\", 14: \"medo\", 15: \"gratidão\", 16: \"luto\", 17: \"alegria\", 18: \"amor\", 19: \"nervosismo\", 20: \"otimismo\", 21: \"orgulho\", 22: \"realização\", 23: \"alívio\", 24: \"remorso\", 25: \"tristeza\", 26: \"surpresa\", 27: \"neutro\"\n",
    "}\n",
    "\n",
    "# Escolher o modelo que deseja usar (base ou large)\n",
    "usar_modelo_large = False  # Alterar para True se quiser usar o modelo large\n",
    "\n",
    "# Carregar o modelo e o tokenizer BERTimbau\n",
    "if usar_modelo_large:\n",
    "    tokenizer = BertTokenizer.from_pretrained(caminho_bertimbau_large)\n",
    "    model = BertForSequenceClassification.from_pretrained(caminho_bertimbau_large)\n",
    "    mapa_sentimentos = mapa_sentimentos_large\n",
    "else:\n",
    "    tokenizer = BertTokenizer.from_pretrained(caminho_bertimbau_base)\n",
    "    model = BertForSequenceClassification.from_pretrained(caminho_bertimbau_base)\n",
    "    mapa_sentimentos = mapa_sentimentos_base\n",
    "\n",
    "# Definir função para remover stopwords personalizadas\n",
    "def remover_stopwords(texto, stopwords_personalizadas):\n",
    "    # Garantir que o texto é uma string e tratar valores nulos\n",
    "    if pd.isnull(texto):\n",
    "        return \"\"\n",
    "    palavras = str(texto).split()\n",
    "    palavras_filtradas = [palavra for palavra in palavras if palavra.lower() not in stopwords_personalizadas]\n",
    "    return ' '.join(palavras_filtradas)\n",
    "\n",
    "# Lista de stopwords personalizadas\n",
    "stopwords_personalizadas = [\n",
    "    '0800', 'a', 'as', 'à', 'às', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'acerca', 'adeus', 'agora', 'ainda', 'algo', 'algumas', 'alguns', 'ali', 'além', 'alô', 'ambos', 'ampla', 'amplas', 'amplo', 'amplos', 'ano', 'ante', 'antes', 'ao', 'aos', 'apenas', 'apoio', 'após', 'aqui', 'assim', 'atrás', 'através', 'até', 'aí', 'baixo', 'bem', 'boa', 'boas', 'bom', 'bons', 'breve', 'cada', 'catorze', 'cedo', 'cento', 'certamente', 'certeza', 'cinco', 'cima', 'coisa', 'coisas', 'com', 'como', 'conselho', 'contudo', 'contra', 'cá', 'da', 'das', 'dado', 'dados', 'dela', 'delas', 'dele', 'deles', 'demais', 'debaixo', 'de', 'dentro', 'depois', 'desde', 'dessa', 'dessas', 'desse', 'desses', 'desta', 'destas', 'deste', 'destes', 'devendo', 'deve', 'devem', 'deverá', 'dez', 'dezoito', 'dia', 'diante', 'disse', 'disso', 'disto', 'dito', 'diz', 'dizem', 'dizer', 'do', 'dos', 'dois', 'doze', 'duas', 'dúvida', 'e', 'é', 'ela', 'elas', 'ele', 'eles', 'embora', 'em', 'empresa', 'empresas', 'enquanto', 'entre', 'então', 'era', 'eram', 'éramos', 'essa', 'essas', 'esse', 'esses', 'está', 'estamos', 'estão', 'esta', 'estar', 'estas', 'estava', 'estavam', 'estávamos', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estivéramos', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéssemos', 'estou', 'eu', 'evento', 'exemplo', 'falta', 'favor', 'faz', 'fazem', 'fazemos', 'fazendo', 'fazer', 'fazeres', 'faço', 'feita', 'feitas', 'feito', 'feitos', 'fim', 'final', 'fomos', 'fôramos', 'fora', 'for', 'foram', 'forma', 'formos', 'fosse', 'fossem', 'fôssemos', 'foi', 'fui', 'geral', 'grande', 'grandes', 'grátis', 'haja', 'hajam', 'hajamos', 'hão', 'haver', 'havemos', 'havia', 'hei', 'hoje', 'hora', 'horas', 'houve', 'houvemos', 'houver', 'houvera', 'houverá', 'houveram', 'houvéramos', 'houverão', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houveríamos', 'houvermos', 'houvesse', 'houvessem', 'houvéssemos', 'há', 'https', 'isso', 'isto', 'já', 'la', 'lado', 'lhe', 'lhes', 'ligado', 'lo', 'local', 'logo', 'longe', 'lugar', 'lá', 'maior', 'maioria', 'maiorias', 'mal', 'mais', 'mas', 'me', 'meio', 'menor', 'menos', 'meses', 'mesma', 'mesmas', 'mesmo', 'mesmos', 'meu', 'meus', 'minha', 'minhas', 'mil', 'milhões', 'milhares', 'muito', 'muitas', 'muitos', 'na', 'nas', 'nada', 'não', 'naquela', 'naquelas', 'naquele', 'naqueles', 'nas', 'negócio', 'negócios', 'nem', 'nesta', 'nestas', 'nessa', 'nessas', 'nesse', 'nesses', 'ninguém', 'nível', 'no', 'nos', 'noite', 'nome', 'nossa', 'nossas', 'nosso', 'nossos', 'nova', 'novas', 'nove', 'novo', 'novos', 'num', 'numa', 'nunca', 'número', 'nós', 'o', 'obra', 'obrigada', 'obrigado', 'oitava', 'oitavo', 'oito', 'onde', 'ontem', 'onze', 'os', 'ou', 'outra', 'outras', 'outro', 'outros', 'para', 'parece', 'parte', 'partir', 'pela', 'pelas', 'pelo', 'pelos', 'pequena', 'pequenas', 'pequeno', 'pequenos', 'per', 'perante', 'perto', 'pode', 'podem', 'podendo', 'poder', 'poderá', 'poderão', 'poderia', 'poderiam', 'podia', 'podiam', 'pois', 'ponto', 'pontos', 'por', 'porque', 'porquê', 'posição', 'possivelmente', 'possível', 'posso', 'pouca', 'poucas', 'pouco', 'poucos', 'primeira', 'primeiras', 'primeiro', 'primeiros', 'própria', 'próprias', 'próprio', 'próprios', 'próxima', 'próximas', 'próximo', 'próximos', 'pude', 'qual', 'quais', 'qualquer', 'quando', 'quanto', 'quantos', 'quarta', 'quarto', 'quatro', 'que', 'quer', 'querem', 'quero', 'quem', 'questão', 'quinta', 'quinto', 'quinze', 'relação', 'sabe', 'sabem', 'se', 'sebrae', 'segunda', 'segundo', 'sei', 'seis', 'seja', 'sejam', 'sem', 'sempre', 'sendo', 'ser', 'será', 'serão', 'seremos', 'seria', 'seriam', 'seríamos', 'sete', 'seu', 'seus', 'sexta', 'sexto', 'si', 'sido', 'sim', 'sistema', 'só', 'sob', 'sobre', 'somente', 'somos', 'sou', 'sua', 'suas', 'são', 'tal', 'talvez', 'também', 'tampouco', 'tanta', 'tantas', 'tarde', 'tanto', 'te', 'tem', 'tém', 'temos', 'tendo', 'tenha', 'tenham', 'tenhamos', 'tenho', 'ter', 'terá', 'terão', 'terceira', 'terceiro', 'terei', 'teremos', 'teria', 'teriam', 'teríamos', 'teve', 'teu', 'teus', 'tido', 'tinha', 'tinham', 'tínhamos', 'tipo', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tivéramos', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéssemos', 'toda', 'todas', 'todo', 'todos', 'trabalho', 'três', 'treze', 'tu', 'tua', 'tuas', 'tudo', 'um', 'uma', 'umas', 'uns', 'usa', 'usar', 'vai', 'vais', 'vem', 'vendo', 'ver', 'vez', 'vezes', 'vindo', 'vir', 'vários', 'você', 'vocês', 'vos', 'vão', 'www', 'à', 'é', 'última', 'últimas', 'último', 'últimos', 'br', 'ac', 'al', 'ap', 'am', 'ba', 'ce', 'df', 'es', 'go', 'ma', 'mt', 'ms', 'mg', 'pa', 'pb', 'pr', 'pe', 'pi', 'rj', 'rn', 'rs', 'ro', 'rr', 'sc', 'sp', 'se', 'to', 'acre', 'alagoas', 'amazonas', 'bahia', 'ceará', 'distrito', 'espírito', 'santo', 'goiás', 'maranhão', 'mato', 'grosso', 'minas', 'gerais', 'pará', 'paraíba', 'paraná', 'pernambuco', 'piauí', 'rio', 'janeiro', 'grande', 'rondônia', 'roraima', 'santa', 'catarina', 'são', 'paulo', 'sergipe', 'tocantins', 'brasil', 'domingo', 'feira', 'sábado'\n",
    "]\n",
    "\n",
    "# Aplicar pré-processamento dos textos: removendo stopwords personalizadas\n",
    "df['Texto_Processado'] = df['Texto'].apply(lambda x: remover_stopwords(x, stopwords_personalizadas))\n",
    "\n",
    "# Definir função para realizar a análise de sentimentos\n",
    "def analisar_sentimento(texto):\n",
    "    # Ignorar avisos específicos\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers\")\n",
    "    \n",
    "    # Tokenizar o texto (Modelo BERT tem um limite de 512 tokens)\n",
    "    inputs = tokenizer(texto, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        \n",
    "    # Desativar o cálculo de gradiente para economizar memória\n",
    "    with torch.no_grad():\n",
    "        # Obter as previsões do modelo\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Aplicar softmax para obter as probabilidades de cada classe\n",
    "    probabilidades = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    \n",
    "    # Determinar a classe de sentimento com maior probabilidade\n",
    "    sentimento = torch.argmax(probabilidades, dim=1).item()\n",
    "    \n",
    "    return sentimento\n",
    "\n",
    "# Aplicar a função de análise de sentimentos ao DataFrame\n",
    "df['Sentimento'] = df['Texto_Processado'].apply(analisar_sentimento)\n",
    "\n",
    "# Mapear o significado dos sentimentos\n",
    "df['Significado_Sentimento'] = df['Sentimento'].map(mapa_sentimentos)\n",
    "\n",
    "# Salvar o DataFrame com os resultados da análise de sentimentos em um novo arquivo Excel\n",
    "df.to_excel('analise_sentimentos_modelo_BERTimbau_base.xlsx', index=False)\n",
    "\n",
    "print(\"Dados salvos no arquivo Excel com sucesso!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf18307-d7da-4995-8fd6-1add58670a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis do DataFrame \"ws_agencia_sebrae_noticias.xlsx\" com o modelo BERTimabu (LARGE)\n",
    "\n",
    "# Importar as bibliotecas necessárias\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import warnings\n",
    "\n",
    "# Caminho do arquivo no computador\n",
    "caminho_arquivo = r'C:\\Users\\Thairone Almeida\\Google Drive\\Thairone\\Hashtag Treinamentos\\Python Impressionador\\Script\\MBA_USP_ESALQ\\Dados até 30 06 2024\\ws_agencia_sebrae_noticias.xlsx'\n",
    "\n",
    "# Caminhos dos modelos BERTimbau\n",
    "caminho_bertimbau_base = r'C:\\Users\\Thairone Almeida\\Google Drive\\Thairone\\Hashtag Treinamentos\\Python Impressionador\\Script\\MBA_USP_ESALQ\\BERTimbau_base_GoEmotions_portuguese'\n",
    "caminho_bertimbau_large = r'C:\\Users\\Thairone Almeida\\Google Drive\\Thairone\\Hashtag Treinamentos\\Python Impressionador\\Script\\MBA_USP_ESALQ\\BERTimbau_large_GoEmotions_portuguese'\n",
    "\n",
    "# Carregar os dados do arquivo Excel\n",
    "df = pd.read_excel(caminho_arquivo)\n",
    "\n",
    "# Mapeamento de BERTimbau Base\n",
    "mapa_sentimentos_base = {\n",
    "    0: \"admiração\", 1: \"diversão\", 2: \"raiva\", 3: \"aborrecimento\", 4: \"aprovação\", 5: \"zelo\", 6: \"confusão\", 7: \"curiosidade\", 8: \"desejo\", 9: \"decepção\", 10: \"desaprovação\", 11: \"nojo\", 12: \"constrangimento\", 13: \"entusiasmo\", 14: \"medo\", 15: \"gratidão\", 16: \"luto\", 17: \"alegria\", 18: \"amor\", 19: \"nervosismo\", 20: \"otimismo\", 21: \"orgulho\", 22: \"percepção\", 23: \"alívio\", 24: \"remorso\", 25: \"tristeza\", 26: \"surpresa\", 27: \"neutro\"\n",
    "}\n",
    "\n",
    "# Mapeamento de BERTimbau Large\n",
    "mapa_sentimentos_large = {\n",
    "    0: \"admiração\", 1: \"diversão\", 2: \"raiva\", 3: \"aborrecimento\", 4: \"aprovação\", 5: \"zelo\", 6: \"confusão\", 7: \"curiosidade\", 8: \"desejo\", 9: \"decepção\", 10: \"desaprovação\", 11: \"nojo\", 12: \"constrangimento\", 13: \"entusiasmo\", 14: \"medo\", 15: \"gratidão\", 16: \"luto\", 17: \"alegria\", 18: \"amor\", 19: \"nervosismo\", 20: \"otimismo\", 21: \"orgulho\", 22: \"realização\", 23: \"alívio\", 24: \"remorso\", 25: \"tristeza\", 26: \"surpresa\", 27: \"neutro\"\n",
    "}\n",
    "\n",
    "# Escolher o modelo que deseja usar (base ou large)\n",
    "usar_modelo_large = True  # Alterar para True se quiser usar o modelo large\n",
    "\n",
    "# Carregar o modelo e o tokenizer BERTimbau\n",
    "if usar_modelo_large:\n",
    "    tokenizer = BertTokenizer.from_pretrained(caminho_bertimbau_large)\n",
    "    model = BertForSequenceClassification.from_pretrained(caminho_bertimbau_large)\n",
    "    mapa_sentimentos = mapa_sentimentos_large\n",
    "else:\n",
    "    tokenizer = BertTokenizer.from_pretrained(caminho_bertimbau_base)\n",
    "    model = BertForSequenceClassification.from_pretrained(caminho_bertimbau_base)\n",
    "    mapa_sentimentos = mapa_sentimentos_base\n",
    "\n",
    "# Definir função para remover stopwords personalizadas\n",
    "def remover_stopwords(texto, stopwords_personalizadas):\n",
    "    # Garantir que o texto é uma string e tratar valores nulos\n",
    "    if pd.isnull(texto):\n",
    "        return \"\"\n",
    "    palavras = str(texto).split()\n",
    "    palavras_filtradas = [palavra for palavra in palavras if palavra.lower() not in stopwords_personalizadas]\n",
    "    return ' '.join(palavras_filtradas)\n",
    "\n",
    "# Lista de stopwords personalizadas\n",
    "stopwords_personalizadas = [\n",
    "    '0800', 'a', 'as', 'à', 'às', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'acerca', 'adeus', 'agora', 'ainda', 'algo', 'algumas', 'alguns', 'ali', 'além', 'alô', 'ambos', 'ampla', 'amplas', 'amplo', 'amplos', 'ano', 'ante', 'antes', 'ao', 'aos', 'apenas', 'apoio', 'após', 'aqui', 'assim', 'atrás', 'através', 'até', 'aí', 'baixo', 'bem', 'boa', 'boas', 'bom', 'bons', 'breve', 'cada', 'catorze', 'cedo', 'cento', 'certamente', 'certeza', 'cinco', 'cima', 'coisa', 'coisas', 'com', 'como', 'conselho', 'contudo', 'contra', 'cá', 'da', 'das', 'dado', 'dados', 'dela', 'delas', 'dele', 'deles', 'demais', 'debaixo', 'de', 'dentro', 'depois', 'desde', 'dessa', 'dessas', 'desse', 'desses', 'desta', 'destas', 'deste', 'destes', 'devendo', 'deve', 'devem', 'deverá', 'dez', 'dezoito', 'dia', 'diante', 'disse', 'disso', 'disto', 'dito', 'diz', 'dizem', 'dizer', 'do', 'dos', 'dois', 'doze', 'duas', 'dúvida', 'e', 'é', 'ela', 'elas', 'ele', 'eles', 'embora', 'em', 'empresa', 'empresas', 'enquanto', 'entre', 'então', 'era', 'eram', 'éramos', 'essa', 'essas', 'esse', 'esses', 'está', 'estamos', 'estão', 'esta', 'estar', 'estas', 'estava', 'estavam', 'estávamos', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estivéramos', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéssemos', 'estou', 'eu', 'evento', 'exemplo', 'falta', 'favor', 'faz', 'fazem', 'fazemos', 'fazendo', 'fazer', 'fazeres', 'faço', 'feita', 'feitas', 'feito', 'feitos', 'fim', 'final', 'fomos', 'fôramos', 'fora', 'for', 'foram', 'forma', 'formos', 'fosse', 'fossem', 'fôssemos', 'foi', 'fui', 'geral', 'grande', 'grandes', 'grátis', 'haja', 'hajam', 'hajamos', 'hão', 'haver', 'havemos', 'havia', 'hei', 'hoje', 'hora', 'horas', 'houve', 'houvemos', 'houver', 'houvera', 'houverá', 'houveram', 'houvéramos', 'houverão', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houveríamos', 'houvermos', 'houvesse', 'houvessem', 'houvéssemos', 'há', 'https', 'isso', 'isto', 'já', 'la', 'lado', 'lhe', 'lhes', 'ligado', 'lo', 'local', 'logo', 'longe', 'lugar', 'lá', 'maior', 'maioria', 'maiorias', 'mal', 'mais', 'mas', 'me', 'meio', 'menor', 'menos', 'meses', 'mesma', 'mesmas', 'mesmo', 'mesmos', 'meu', 'meus', 'minha', 'minhas', 'mil', 'milhões', 'milhares', 'muito', 'muitas', 'muitos', 'na', 'nas', 'nada', 'não', 'naquela', 'naquelas', 'naquele', 'naqueles', 'nas', 'negócio', 'negócios', 'nem', 'nesta', 'nestas', 'nessa', 'nessas', 'nesse', 'nesses', 'ninguém', 'nível', 'no', 'nos', 'noite', 'nome', 'nossa', 'nossas', 'nosso', 'nossos', 'nova', 'novas', 'nove', 'novo', 'novos', 'num', 'numa', 'nunca', 'número', 'nós', 'o', 'obra', 'obrigada', 'obrigado', 'oitava', 'oitavo', 'oito', 'onde', 'ontem', 'onze', 'os', 'ou', 'outra', 'outras', 'outro', 'outros', 'para', 'parece', 'parte', 'partir', 'pela', 'pelas', 'pelo', 'pelos', 'pequena', 'pequenas', 'pequeno', 'pequenos', 'per', 'perante', 'perto', 'pode', 'podem', 'podendo', 'poder', 'poderá', 'poderão', 'poderia', 'poderiam', 'podia', 'podiam', 'pois', 'ponto', 'pontos', 'por', 'porque', 'porquê', 'posição', 'possivelmente', 'possível', 'posso', 'pouca', 'poucas', 'pouco', 'poucos', 'primeira', 'primeiras', 'primeiro', 'primeiros', 'própria', 'próprias', 'próprio', 'próprios', 'próxima', 'próximas', 'próximo', 'próximos', 'pude', 'qual', 'quais', 'qualquer', 'quando', 'quanto', 'quantos', 'quarta', 'quarto', 'quatro', 'que', 'quer', 'querem', 'quero', 'quem', 'questão', 'quinta', 'quinto', 'quinze', 'relação', 'sabe', 'sabem', 'se', 'sebrae', 'segunda', 'segundo', 'sei', 'seis', 'seja', 'sejam', 'sem', 'sempre', 'sendo', 'ser', 'será', 'serão', 'seremos', 'seria', 'seriam', 'seríamos', 'sete', 'seu', 'seus', 'sexta', 'sexto', 'si', 'sido', 'sim', 'sistema', 'só', 'sob', 'sobre', 'somente', 'somos', 'sou', 'sua', 'suas', 'são', 'tal', 'talvez', 'também', 'tampouco', 'tanta', 'tantas', 'tarde', 'tanto', 'te', 'tem', 'tém', 'temos', 'tendo', 'tenha', 'tenham', 'tenhamos', 'tenho', 'ter', 'terá', 'terão', 'terceira', 'terceiro', 'terei', 'teremos', 'teria', 'teriam', 'teríamos', 'teve', 'teu', 'teus', 'tido', 'tinha', 'tinham', 'tínhamos', 'tipo', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tivéramos', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéssemos', 'toda', 'todas', 'todo', 'todos', 'trabalho', 'três', 'treze', 'tu', 'tua', 'tuas', 'tudo', 'um', 'uma', 'umas', 'uns', 'usa', 'usar', 'vai', 'vais', 'vem', 'vendo', 'ver', 'vez', 'vezes', 'vindo', 'vir', 'vários', 'você', 'vocês', 'vos', 'vão', 'www', 'à', 'é', 'última', 'últimas', 'último', 'últimos', 'br', 'ac', 'al', 'ap', 'am', 'ba', 'ce', 'df', 'es', 'go', 'ma', 'mt', 'ms', 'mg', 'pa', 'pb', 'pr', 'pe', 'pi', 'rj', 'rn', 'rs', 'ro', 'rr', 'sc', 'sp', 'se', 'to', 'acre', 'alagoas', 'amazonas', 'bahia', 'ceará', 'distrito', 'espírito', 'santo', 'goiás', 'maranhão', 'mato', 'grosso', 'minas', 'gerais', 'pará', 'paraíba', 'paraná', 'pernambuco', 'piauí', 'rio', 'janeiro', 'grande', 'rondônia', 'roraima', 'santa', 'catarina', 'são', 'paulo', 'sergipe', 'tocantins', 'brasil', 'domingo', 'feira', 'sábado'\n",
    "]\n",
    "\n",
    "# Aplicar pré-processamento dos textos: removendo stopwords personalizadas\n",
    "df['Texto_Processado'] = df['Texto'].apply(lambda x: remover_stopwords(x, stopwords_personalizadas))\n",
    "\n",
    "# Definir função para realizar a análise de sentimentos\n",
    "def analisar_sentimento(texto):\n",
    "    # Ignorar avisos específicos\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers\")\n",
    "    \n",
    "    # Tokenizar o texto (Modelo BERT tem um limite de 512 tokens)\n",
    "    inputs = tokenizer(texto, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        \n",
    "    # Desativar o cálculo de gradiente para economizar memória\n",
    "    with torch.no_grad():\n",
    "        # Obter as previsões do modelo\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Aplicar softmax para obter as probabilidades de cada classe\n",
    "    probabilidades = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    \n",
    "    # Determinar a classe de sentimento com maior probabilidade\n",
    "    sentimento = torch.argmax(probabilidades, dim=1).item()\n",
    "    \n",
    "    return sentimento\n",
    "\n",
    "# Aplicar a função de análise de sentimentos ao DataFrame\n",
    "df['Sentimento'] = df['Texto_Processado'].apply(analisar_sentimento)\n",
    "\n",
    "# Mapear o significado dos sentimentos\n",
    "df['Significado_Sentimento'] = df['Sentimento'].map(mapa_sentimentos)\n",
    "\n",
    "# Salvar o DataFrame com os resultados da análise de sentimentos em um novo arquivo Excel\n",
    "df.to_excel('analise_sentimentos_modelo_BERTimbau_large.xlsx', index=False)\n",
    "\n",
    "print(\"Dados salvos no arquivo Excel com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
