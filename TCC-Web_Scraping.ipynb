{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10457960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping da Agência SEBRAE de Notícias\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Dicionário de estados e suas respectivas URLs\n",
    "urls_estados = {\n",
    "    \"BR\": \"https://agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"AC\": \"https://ac.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"AL\": \"https://al.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"AM\": \"https://am.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"AP\": \"https://ap.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"BA\": \"https://ba.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"CE\": \"https://ce.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"DF\": \"https://df.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"ES\": \"https://es.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"GO\": \"https://go.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"MA\": \"https://ma.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"MG\": \"https://mg.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"MS\": \"https://ms.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"MT\": \"https://mt.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"PA\": \"https://pa.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"PB\": \"https://pb.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"PE\": \"https://pe.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"PI\": \"https://pi.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"PR\": \"https://pr.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"RJ\": \"https://rj.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"RN\": \"https://rn.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"RO\": \"https://ro.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"RR\": \"https://rr.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"RS\": \"https://rs.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"SC\": \"https://sc.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"SE\": \"https://se.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"SP\": \"https://sp.agenciasebrae.com.br/ultimas-noticias/page/\",\n",
    "    \"TO\": \"https://to.agenciasebrae.com.br/ultimas-noticias/page/\"\n",
    "}\n",
    "\n",
    "# Listas para armazenar os dados extraídos\n",
    "estados = []  # Armazenar o estado onde foi publicada cada notícia\n",
    "links = []  # Armazenar o link de cada notícia\n",
    "datas = []  # Armazenar a data de publicação de cada notícia\n",
    "editoriais = []  # Armazenar o editorial de cada notícia\n",
    "titulos = []  # Armazenar o título de cada notícia\n",
    "subtitulos = []  # Armazenar o subtítulo de cada notícia\n",
    "textos = []  # Armazenar o texto de cada notícia\n",
    "\n",
    "# Loop para web scraping da página de notícias de cada estado\n",
    "for estado, base_url in urls_estados.items():\n",
    "    # Iniciar a contagem de páginas a partir da primeira\n",
    "    pagina_inicial = 1\n",
    "    \n",
    "    # Criar um loop contínuo até ser interrompido\n",
    "    while True:\n",
    "        url = base_url + str(pagina_inicial) + \"/\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Verificar se a página de erro foi carregada (interrupção)\n",
    "        error_message = soup.find('div', class_='info')\n",
    "        if error_message and 'Não encontramos essa página' in error_message.text:\n",
    "            print(f'Página de erro encontrada em {url}. Parando a busca.')\n",
    "            break\n",
    "\n",
    "        # Extrair estado e link de cada notícia\n",
    "        for h3 in soup.find_all('h3', class_='title'):\n",
    "            estados.append(estado)  # Adiciona o estado correspondente\n",
    "            links.append(h3.find('a')['href'])  # Adiciona o link correspondente\n",
    "        \n",
    "        # Incrementar o número da página para a próxima iteração\n",
    "        pagina_inicial += 1\n",
    "\n",
    "# Extrair data de publicação, editorial, título, subtítulo e texto de cada notícia\n",
    "for link in links:\n",
    "    # Tratar erro de redirecionamento excessivo\n",
    "    try:\n",
    "        response_noticia = requests.get(link)\n",
    "    except requests.TooManyRedirects:\n",
    "        print(f\"Erro de redirecionamento excessivo para {link}.\")\n",
    "        error_message = \"Erro de redirecionamento excessivo\"\n",
    "        datas.append(error_message)\n",
    "        editoriais.append(error_message)\n",
    "        titulos.append(error_message)\n",
    "        subtitulos.append(error_message)\n",
    "        textos.append(error_message)\n",
    "        continue\n",
    "\n",
    "    soup_noticia = BeautifulSoup(response_noticia.text, 'html.parser')\n",
    "\n",
    "    # Extrair a data de publicação\n",
    "    time_tag = soup_noticia.find('time', {'data-datetime': True})\n",
    "    if time_tag:\n",
    "        datas.append(time_tag['data-datetime'])\n",
    "    else:\n",
    "        datas.append(\"Data não disponível\")\n",
    "\n",
    "    # Extrair o editorial\n",
    "    editorial_tag = soup_noticia.find('div', class_='editoria-crumb')\n",
    "    if editorial_tag:\n",
    "        editoriais.append(editorial_tag.get_text(strip=True))\n",
    "    else:\n",
    "        editoriais.append(\"Editorial não disponível\")\n",
    "\n",
    "    # Extrair o título\n",
    "    titulo_tag = soup_noticia.find('h2')\n",
    "    if titulo_tag:\n",
    "        titulos.append(titulo_tag.text.strip())\n",
    "    else:\n",
    "        titulos.append(\"Título não disponível\")\n",
    "\n",
    "    # Extrair o subtítulo\n",
    "    subtitulo_tag = soup_noticia.find('em', class_='sub-title')\n",
    "    if subtitulo_tag:\n",
    "        subtitulos.append(subtitulo_tag.text.strip())\n",
    "    else:\n",
    "        subtitulos.append(\"Subtítulo não disponível\")\n",
    "\n",
    "    # Extrair o texto\n",
    "    paragraphs = [p.text for p in soup_noticia.select('.text-content p')]\n",
    "    textos.append('\\n'.join(paragraphs))\n",
    "\n",
    "# Extrair tamanho de cada lista (teste)\n",
    "print(\"Tamanho de cada lista: {}, {}, {}, {}, {}, {}, {}\".format(len(estados), len(links), len(datas), len(editoriais), len(titulos), len(subtitulos), len(textos)))\n",
    "\n",
    "# Criar DataFrame a partir das listas\n",
    "df = pd.DataFrame({\n",
    "    'Estado': estados,\n",
    "    'Link': links,\n",
    "    'Data de Publicação': datas,\n",
    "    'Editorial': editoriais,\n",
    "    'Título': titulos,\n",
    "    'Subtítulo': subtitulos,\n",
    "    'Texto': textos\n",
    "})\n",
    "\n",
    "# Salvar DataFrame em um arquivo\n",
    "df.to_excel('ws_agencia_sebrae_noticias.xlsx', index=False)\n",
    "\n",
    "print(\"Dados salvos no arquivo Excel com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
